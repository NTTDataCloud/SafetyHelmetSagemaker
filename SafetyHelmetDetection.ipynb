{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::467343721842:role/service-role/AmazonSageMaker-ExecutionRole-20200521T183987\n",
      "811284229777.dkr.ecr.us-east-1.amazonaws.com/object-detection:latest\n",
      "CPU times: user 646 ms, sys: 80.5 ms, total: 727 ms\n",
      "Wall time: 770 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "sess = sagemaker.Session()\n",
    "bucket = 'safetyhelmettestbucket' # custom bucket name.\n",
    "# bucket = sess.default_bucket()\n",
    "prefix = 'objectDetection'\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "training_image = get_image_uri(sess.boto_region_name, 'object-detection', repo_version=\"latest\")\n",
    "print (training_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf train\n",
    "rm -rf train_annotation\n",
    "rm -rf val\n",
    "rm -rf val_annotation\n",
    "mkdir train\n",
    "mkdir train_annotation\n",
    "mkdir val\n",
    "mkdir val_annotation\n",
    "cp JPEGImages/* train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import os\n",
    "import json\n",
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "mypath = \"Annotations\"\n",
    "jsonpath = \"train_annotation\"\n",
    "\n",
    "files = listdir(mypath)\n",
    "\n",
    "for f in files:\n",
    "    \n",
    "    fullfilename = mypath + '/' + f\n",
    "    jsonfullfilename = jsonpath + '/' + f.replace(\"xml\",\"json\")\n",
    "    line = {}\n",
    "    \n",
    "    tree = ET.ElementTree(file=fullfilename)\n",
    "    root = tree.getroot()\n",
    "    categories = {}\n",
    "    for child in root:\n",
    "    \n",
    "        if child.tag == \"filename\":\n",
    "            line[\"file\"] = f.replace(\"xml\",\"jpg\")\n",
    "        if child.tag == \"size\":\n",
    "            line[\"image_size\"] = []\n",
    "            image_size = {}\n",
    "            for s in child:\n",
    "                if s.tag == \"width\":\n",
    "                    image_size[\"width\"] = int(s.text)\n",
    "                elif s.tag == \"height\":\n",
    "                    image_size[\"height\"] = int(s.text)\n",
    "                elif s.tag == \"depth\":\n",
    "                    image_size[\"depth\"] = int(s.text)\n",
    "                else:\n",
    "                    print(s.tag,\":\",s.text)\n",
    "        \n",
    "            line[\"image_size\"].append(image_size)\n",
    "            line[\"annotations\"] = []            \n",
    "            line[\"categories\"] = []\n",
    "            \n",
    "        if child.tag == \"object\":\n",
    "            annotation = {}\n",
    "            for s in child:\n",
    "                                            \n",
    "                if s.tag == \"bndbox\": \n",
    "                    left = 0\n",
    "                    right = 0\n",
    "                    top = 0\n",
    "                    bottom = 0\n",
    "                    for ss in s:\n",
    "                        if ss.tag == \"xmin\":\n",
    "                            left = int(ss.text)\n",
    "                        elif ss.tag == \"xmax\":\n",
    "                            right = int(ss.text)\n",
    "                        elif ss.tag == \"ymin\":\n",
    "                            top = int(ss.text)\n",
    "                        elif ss.tag == \"ymax\":\n",
    "                            bottom = int(ss.text)\n",
    "                    annotation[\"left\"] = left\n",
    "                    annotation[\"top\"] = top\n",
    "                    annotation[\"width\"] = right - left\n",
    "                    annotation[\"height\"] = bottom - top\n",
    "                if s.tag == \"name\":\n",
    "                    category = {}\n",
    "                    if s.text == \"hat\":\n",
    "                        annotation[\"class_id\"] = 0\n",
    "                        category = {\"class_id\": 0, \"name\": s.text}\n",
    "                    elif s.text == \"person\":\n",
    "                        annotation[\"class_id\"] = 1\n",
    "                        category = {\"class_id\": 1, \"name\": s.text}\n",
    "                    else:\n",
    "                        annotation[\"class_id\"] = 0\n",
    "                        category = {\"class_id\": 0, \"name\": \"hat\"}\n",
    "                    if not category in line[\"categories\"]:\n",
    "                        line[\"categories\"].append(category)\n",
    "            line[\"annotations\"].append(annotation)\n",
    "            \n",
    "    if len(line[\"annotations\"]) > 0 :\n",
    "        with open(jsonfullfilename,\"w\") as p:\n",
    "            json.dump(line,p)\n",
    "    else:\n",
    "        os.remove('train/' + f.replace(\"xml\",\"jpg\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "json_files = os.listdir('train_annotation')\n",
    "\n",
    "m = 4\n",
    "i = 0\n",
    "for f in json_files:\n",
    "    i = i + 1\n",
    "    if i%m == 0:\n",
    "        shutil.move(jsonpath + '/' + f, 'val_annotation/')\n",
    "        filePath = f.replace(\"json\",\"jpg\")\n",
    "        if not os.path.exists('train/' + filePath):\n",
    "            filePath = f.replace(\"json\",\"JPG\")\n",
    "        shutil.move('train/' + filePath, 'val/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5686 files in training folder.\n",
      "1895 files in val folder.\n"
     ]
    }
   ],
   "source": [
    "print ( \"%d files in training folder.\" % len(os.listdir('train_annotation')))\n",
    "print ( \"%d files in val folder.\" % len(os.listdir('val_annotation')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_channel = prefix + '/train'\n",
    "validation_channel = prefix + '/validation'\n",
    "train_annotation_channel = prefix + '/train_annotation'\n",
    "validation_annotation_channel = prefix + '/validation_annotation'\n",
    "\n",
    "sess.upload_data(path='train', bucket=bucket, key_prefix=train_channel)\n",
    "sess.upload_data(path='val', bucket=bucket, key_prefix=validation_channel)\n",
    "sess.upload_data(path='train_annotation', bucket=bucket, key_prefix=train_annotation_channel)\n",
    "sess.upload_data(path='val_annotation', bucket=bucket, key_prefix=validation_annotation_channel)\n",
    "\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, train_channel)\n",
    "s3_validation_data = 's3://{}/{}'.format(bucket, validation_channel)\n",
    "s3_train_annotation = 's3://{}/{}'.format(bucket, train_annotation_channel)\n",
    "s3_validation_annotation = 's3://{}/{}'.format(bucket, validation_annotation_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "od_model = sagemaker.estimator.Estimator(training_image,\n",
    "                                         role, \n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.p3.2xlarge',\n",
    "                                         train_volume_size = 50,\n",
    "                                         train_max_run = 360000,\n",
    "                                         input_mode = 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "od_model.set_hyperparameters(base_network='resnet-50',\n",
    "                             use_pretrained_model=1,\n",
    "                             num_classes=2,\n",
    "                             mini_batch_size=16,\n",
    "                             epochs=100,\n",
    "                             learning_rate=0.01,\n",
    "                             lr_scheduler_step='20,50,80',\n",
    "                             lr_scheduler_factor=0.1,\n",
    "                             optimizer='sgd',\n",
    "                             momentum=0.9,\n",
    "                             weight_decay=0.0005,\n",
    "                             overlap_threshold=0.5,\n",
    "                             nms_threshold=0.45,\n",
    "                             image_shape=512,\n",
    "                             label_width=1600,\n",
    "                             num_training_samples=5686)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.session.s3_input(s3_train_data, distribution='FullyReplicated', \n",
    "                        content_type='image/jpeg', s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3_validation_data, distribution='FullyReplicated', \n",
    "                             content_type='image/jpeg', s3_data_type='S3Prefix')\n",
    "train_annotation = sagemaker.session.s3_input(s3_train_annotation, distribution='FullyReplicated', \n",
    "                             content_type='image/jpeg', s3_data_type='S3Prefix')\n",
    "validation_annotation = sagemaker.session.s3_input(s3_validation_annotation, distribution='FullyReplicated', \n",
    "                             content_type='image/jpeg', s3_data_type='S3Prefix')\n",
    "\n",
    "data_channels = {'train': train_data, 'validation': validation_data, \n",
    "                 'train_annotation': train_annotation, 'validation_annotation':validation_annotation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-29 02:09:57 Starting - Starting the training job...\n",
      "2020-05-29 02:10:01 Starting - Launching requested ML instances.........\n",
      "2020-05-29 02:11:47 Starting - Preparing the instances for training......\n",
      "2020-05-29 02:12:45 Downloading - Downloading input data..................\n",
      "2020-05-29 02:15:53 Training - Downloading the training image...\n",
      "2020-05-29 02:16:13 Training - Training image download completed. Training in progress.\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:16:17 INFO 139786247001920] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'label_width': u'350', u'early_stopping_min_epochs': u'10', u'epochs': u'30', u'overlap_threshold': u'0.5', u'lr_scheduler_factor': u'0.1', u'_num_kv_servers': u'auto', u'weight_decay': u'0.0005', u'mini_batch_size': u'32', u'use_pretrained_model': u'0', u'freeze_layer_pattern': u'', u'lr_scheduler_step': u'', u'early_stopping': u'False', u'early_stopping_patience': u'5', u'momentum': u'0.9', u'num_training_samples': u'', u'optimizer': u'sgd', u'_tuning_objective_metric': u'', u'early_stopping_tolerance': u'0.0', u'learning_rate': u'0.001', u'kv_store': u'device', u'nms_threshold': u'0.45', u'num_classes': u'', u'base_network': u'vgg-16', u'nms_topk': u'400', u'_kvstore': u'device', u'image_shape': u'300'}\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:16:17 INFO 139786247001920] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'0.01', u'epochs': u'100', u'nms_threshold': u'0.45', u'optimizer': u'sgd', u'base_network': u'resnet-50', u'image_shape': u'512', u'label_width': u'1600', u'lr_scheduler_step': u'20,50,80', u'momentum': u'0.9', u'overlap_threshold': u'0.5', u'num_training_samples': u'5686', u'mini_batch_size': u'16', u'weight_decay': u'0.0005', u'use_pretrained_model': u'1', u'num_classes': u'2', u'lr_scheduler_factor': u'0.1'}\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:16:17 INFO 139786247001920] Final configuration: {u'label_width': u'1600', u'early_stopping_min_epochs': u'10', u'epochs': u'100', u'overlap_threshold': u'0.5', u'lr_scheduler_factor': u'0.1', u'_num_kv_servers': u'auto', u'weight_decay': u'0.0005', u'mini_batch_size': u'16', u'use_pretrained_model': u'1', u'freeze_layer_pattern': u'', u'lr_scheduler_step': u'20,50,80', u'early_stopping': u'False', u'early_stopping_patience': u'5', u'momentum': u'0.9', u'num_training_samples': u'5686', u'optimizer': u'sgd', u'_tuning_objective_metric': u'', u'early_stopping_tolerance': u'0.0', u'learning_rate': u'0.01', u'kv_store': u'device', u'nms_threshold': u'0.45', u'num_classes': u'2', u'base_network': u'resnet-50', u'nms_topk': u'400', u'_kvstore': u'device', u'image_shape': u'512'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:16:17 INFO 139786247001920] Using default worker.\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:16:17 INFO 139786247001920] Loaded iterator creator application/x-image for content type ('application/x-image', '1.0')\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:16:17 INFO 139786247001920] Loaded iterator creator application/x-recordio for content type ('application/x-recordio', '1.0')\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:16:17 INFO 139786247001920] Loaded iterator creator image/png for content type ('image/png', '1.0')\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:16:17 INFO 139786247001920] Loaded iterator creator image/jpeg for content type ('image/jpeg', '1.0')\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:16:17 INFO 139786247001920] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:16:20 INFO 139786247001920] nvidia-smi took: 0.0251228809357 secs to identify 1 gpus\u001b[0m\n",
      "\u001b[34mCreating .rec file from /opt/ml/input/data/train/train.lst in /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mtime: 0.0219509601593  count: 0\u001b[0m\n",
      "\u001b[34mtime: 5.72647690773  count: 1000\u001b[0m\n",
      "\u001b[34mtime: 5.4596850872  count: 2000\u001b[0m\n",
      "\u001b[34mtime: 4.78245592117  count: 3000\u001b[0m\n",
      "\u001b[34mtime: 3.63779020309  count: 4000\u001b[0m\n",
      "\u001b[34mtime: 4.32477092743  count: 5000\u001b[0m\n",
      "\u001b[34mCreating .rec file from /opt/ml/input/data/validation/val.lst in /opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mtime: 0.0209889411926  count: 0\u001b[0m\n",
      "\u001b[34mtime: 5.33998608589  count: 1000\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:16:57 INFO 139786247001920] Number of GPUs being used: 1\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:16:57 WARNING 139786247001920] Training images are resized to image shape (3, 512, 512)\u001b[0m\n",
      "\u001b[34m[02:16:57] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-v1.4.1.1457.0/AL2012/generic-flavor/src/src/io/iter_image_det_recordio.cc:283: ImageDetRecordIOParser: /opt/ml/input/data/train/train.rec, use 7 threads for decoding..\u001b[0m\n",
      "\u001b[34m[02:16:57] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-v1.4.1.1457.0/AL2012/generic-flavor/src/src/io/iter_image_det_recordio.cc:340: ImageDetRecordIOParser: /opt/ml/input/data/train/train.rec, label padding width: 1600\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:17:00 WARNING 139786247001920] Validation images are resized to image shape (3, 512, 512)\u001b[0m\n",
      "\u001b[34m[02:17:00] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-v1.4.1.1457.0/AL2012/generic-flavor/src/src/io/iter_image_det_recordio.cc:283: ImageDetRecordIOParser: /opt/ml/input/data/validation/val.rec, use 7 threads for decoding..\u001b[0m\n",
      "\u001b[34m[02:17:00] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-v1.4.1.1457.0/AL2012/generic-flavor/src/src/io/iter_image_det_recordio.cc:340: ImageDetRecordIOParser: /opt/ml/input/data/validation/val.rec, label padding width: 1600\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:17:02 INFO 139786247001920] Number of GPUs being used: 1\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:17:02 INFO 139786247001920] Using [gpu(0)] as training context.\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:17:02 INFO 139786247001920] Number of GPUs being used: 1\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:17:02 INFO 139786247001920] Create Store: device\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:17:02 INFO 139786247001920] Using (gpu(0)) as training context.\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:17:02 INFO 139786247001920] Start training from pretrained model 1.\u001b[0m\n",
      "\u001b[34m[02:17:02] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-v1.4.1.1457.0/AL2012/generic-flavor/src/src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...\u001b[0m\n",
      "\u001b[34m[02:17:02] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-v1.4.1.1457.0/AL2012/generic-flavor/src/src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:17:03 INFO 139786247001920] Loaded pretrained model parameters.\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:17:19 INFO 139786247001920] Creating a new state instance.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1590718639.043057, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Object Detection\"}, \"StartTime\": 1590718639.042969}\n",
      "\u001b[0m\n",
      "\u001b[34m[02:17:19] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-v1.4.1.1457.0/AL2012/generic-flavor/src/src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:18:02 INFO 139786247001920] Epoch:    0, batches:    100, num_examples:   1600, 36.4 samples/sec, epoch time so far:  0:00:43.915641\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:18:44 INFO 139786247001920] Epoch:    0, batches:    200, num_examples:   3200, 37.5 samples/sec, epoch time so far:  0:01:25.388222\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:19:26 INFO 139786247001920] Epoch:    0, batches:    300, num_examples:   4800, 37.6 samples/sec, epoch time so far:  0:02:07.717768\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:19:48 WARNING 139786247001920] Expected number of batches: 355, did not match the number of batches processed: 356. This may happen when some images or annotations are invalid and cannot be parsed. Please check the dataset and ensure it follows the format in the documentation.\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:19:48 INFO 139786247001920] #quality_metric: host=algo-1, epoch=0, batch=356 train cross_entropy <loss>=(0.661748634811741)\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:19:48 INFO 139786247001920] #quality_metric: host=algo-1, epoch=0, batch=356 train smooth_l1 <loss>=(0.7859692376996041)\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:19:48 INFO 139786247001920] Round of batches complete\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:19:48 INFO 139786247001920] Updated the metrics\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:20:29 INFO 139786247001920] #quality_metric: host=algo-1, epoch=0, validation mAP <score>=(0.39739611231454847)\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:20:29 INFO 139786247001920] Updating the best model with validation-mAP=0.39739611231454847\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:20:29 INFO 139786247001920] Saved checkpoint to \"/opt/ml/model/model_algo_1-0000.params\"\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:20:29 INFO 139786247001920] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1590718829.67478, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Object Detection\", \"epoch\": 0}, \"StartTime\": 1590718639.043396}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:21:09 INFO 139786247001920] Epoch:    1, batches:    100, num_examples:   1600, 40.5 samples/sec, epoch time so far:  0:00:39.537548\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:21:50 INFO 139786247001920] Epoch:    1, batches:    200, num_examples:   3200, 39.4 samples/sec, epoch time so far:  0:01:21.299598\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:22:34 INFO 139786247001920] Epoch:    1, batches:    300, num_examples:   4800, 38.5 samples/sec, epoch time so far:  0:02:04.575766\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:22:55 INFO 139786247001920] #quality_metric: host=algo-1, epoch=1, batch=355 train cross_entropy <loss>=(0.5145059928566008)\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:22:55 INFO 139786247001920] #quality_metric: host=algo-1, epoch=1, batch=355 train smooth_l1 <loss>=(0.5444255562944893)\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:22:55 INFO 139786247001920] Round of batches complete\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:22:55 INFO 139786247001920] Updated the metrics\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:23:35 INFO 139786247001920] #quality_metric: host=algo-1, epoch=1, validation mAP <score>=(0.42968255930345567)\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:23:35 INFO 139786247001920] Updating the best model with validation-mAP=0.42968255930345567\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:23:35 INFO 139786247001920] Saved checkpoint to \"/opt/ml/model/model_algo_1-0000.params\"\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:23:35 INFO 139786247001920] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1590719015.303228, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Object Detection\", \"epoch\": 1}, \"StartTime\": 1590718829.674966}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:24:15 INFO 139786247001920] Epoch:    2, batches:    100, num_examples:   1600, 39.3 samples/sec, epoch time so far:  0:00:40.673334\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:24:58 INFO 139786247001920] Epoch:    2, batches:    200, num_examples:   3200, 38.4 samples/sec, epoch time so far:  0:01:23.359536\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:25:39 INFO 139786247001920] Epoch:    2, batches:    300, num_examples:   4800, 38.6 samples/sec, epoch time so far:  0:02:04.353180\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:26:01 WARNING 139786247001920] Expected number of batches: 355, did not match the number of batches processed: 356. This may happen when some images or annotations are invalid and cannot be parsed. Please check the dataset and ensure it follows the format in the documentation.\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:26:01 INFO 139786247001920] #quality_metric: host=algo-1, epoch=2, batch=356 train cross_entropy <loss>=(0.48068062640210707)\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:26:01 INFO 139786247001920] #quality_metric: host=algo-1, epoch=2, batch=356 train smooth_l1 <loss>=(0.49556834198501315)\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:26:01 INFO 139786247001920] Round of batches complete\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:26:02 INFO 139786247001920] Updated the metrics\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:26:40 INFO 139786247001920] #quality_metric: host=algo-1, epoch=2, validation mAP <score>=(0.47890343822474507)\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:26:40 INFO 139786247001920] Updating the best model with validation-mAP=0.47890343822474507\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:26:40 INFO 139786247001920] Saved checkpoint to \"/opt/ml/model/model_algo_1-0000.params\"\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:26:40 INFO 139786247001920] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 3, \"sum\": 3.0, \"min\": 3}}, \"EndTime\": 1590719200.761738, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Object Detection\", \"epoch\": 2}, \"StartTime\": 1590719015.303447}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:27:22 INFO 139786247001920] Epoch:    3, batches:    100, num_examples:   1600, 38.3 samples/sec, epoch time so far:  0:00:41.760008\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:28:07 INFO 139786247001920] Epoch:    3, batches:    200, num_examples:   3200, 37.1 samples/sec, epoch time so far:  0:01:26.321125\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:28:49 INFO 139786247001920] Epoch:    3, batches:    300, num_examples:   4800, 37.2 samples/sec, epoch time so far:  0:02:09.202171\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:29:12 INFO 139786247001920] #quality_metric: host=algo-1, epoch=3, batch=355 train cross_entropy <loss>=(0.45945525128559267)\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:29:12 INFO 139786247001920] #quality_metric: host=algo-1, epoch=3, batch=355 train smooth_l1 <loss>=(0.484968638612617)\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:29:12 INFO 139786247001920] Round of batches complete\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:29:12 INFO 139786247001920] Updated the metrics\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:29:51 INFO 139786247001920] #quality_metric: host=algo-1, epoch=3, validation mAP <score>=(0.46565005235549717)\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:29:51 INFO 139786247001920] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1590719391.039339, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Object Detection\", \"epoch\": 3}, \"StartTime\": 1590719200.76197}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:30:31 INFO 139786247001920] Epoch:    4, batches:    100, num_examples:   1600, 39.3 samples/sec, epoch time so far:  0:00:40.706401\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:31:14 INFO 139786247001920] Epoch:    4, batches:    200, num_examples:   3200, 38.2 samples/sec, epoch time so far:  0:01:23.804336\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:31:57 INFO 139786247001920] Epoch:    4, batches:    300, num_examples:   4800, 38.0 samples/sec, epoch time so far:  0:02:06.209755\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:32:20 INFO 139786247001920] #quality_metric: host=algo-1, epoch=4, batch=355 train cross_entropy <loss>=(0.4431788180476777)\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:32:20 INFO 139786247001920] #quality_metric: host=algo-1, epoch=4, batch=355 train smooth_l1 <loss>=(0.451695709133137)\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:32:20 INFO 139786247001920] Round of batches complete\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:32:20 INFO 139786247001920] Updated the metrics\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:32:59 INFO 139786247001920] #quality_metric: host=algo-1, epoch=4, validation mAP <score>=(0.4781235391800073)\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:32:59 INFO 139786247001920] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 5, \"sum\": 5.0, \"min\": 5}}, \"EndTime\": 1590719579.372158, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Object Detection\", \"epoch\": 4}, \"StartTime\": 1590719391.039537}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:33:40 INFO 139786247001920] Epoch:    5, batches:    100, num_examples:   1600, 39.1 samples/sec, epoch time so far:  0:00:40.884662\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:34:23 INFO 139786247001920] Epoch:    5, batches:    200, num_examples:   3200, 38.0 samples/sec, epoch time so far:  0:01:24.221823\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:35:07 INFO 139786247001920] Epoch:    5, batches:    300, num_examples:   4800, 37.3 samples/sec, epoch time so far:  0:02:08.598554\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:35:30 WARNING 139786247001920] Expected number of batches: 355, did not match the number of batches processed: 356. This may happen when some images or annotations are invalid and cannot be parsed. Please check the dataset and ensure it follows the format in the documentation.\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:35:30 INFO 139786247001920] #quality_metric: host=algo-1, epoch=5, batch=356 train cross_entropy <loss>=(0.43340154635161593)\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:35:30 INFO 139786247001920] #quality_metric: host=algo-1, epoch=5, batch=356 train smooth_l1 <loss>=(0.45207605337835)\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:35:30 INFO 139786247001920] Round of batches complete\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:35:30 INFO 139786247001920] Updated the metrics\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:36:08 INFO 139786247001920] #quality_metric: host=algo-1, epoch=5, validation mAP <score>=(0.49021145039982583)\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:36:08 INFO 139786247001920] Updating the best model with validation-mAP=0.49021145039982583\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:36:08 INFO 139786247001920] Saved checkpoint to \"/opt/ml/model/model_algo_1-0000.params\"\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:36:08 INFO 139786247001920] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1590719768.472367, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Object Detection\", \"epoch\": 5}, \"StartTime\": 1590719579.372366}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:36:52 INFO 139786247001920] Epoch:    6, batches:    100, num_examples:   1600, 36.7 samples/sec, epoch time so far:  0:00:43.544063\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:37:33 INFO 139786247001920] Epoch:    6, batches:    200, num_examples:   3200, 37.7 samples/sec, epoch time so far:  0:01:24.914963\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:38:15 INFO 139786247001920] Epoch:    6, batches:    300, num_examples:   4800, 37.9 samples/sec, epoch time so far:  0:02:06.655524\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:38:38 INFO 139786247001920] #quality_metric: host=algo-1, epoch=6, batch=355 train cross_entropy <loss>=(0.42752463368510935)\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:38:38 INFO 139786247001920] #quality_metric: host=algo-1, epoch=6, batch=355 train smooth_l1 <loss>=(0.4276665962437782)\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:38:38 INFO 139786247001920] Round of batches complete\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:38:38 INFO 139786247001920] Updated the metrics\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:39:18 INFO 139786247001920] #quality_metric: host=algo-1, epoch=6, validation mAP <score>=(0.5049687218510689)\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:39:18 INFO 139786247001920] Updating the best model with validation-mAP=0.5049687218510689\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:39:18 INFO 139786247001920] Saved checkpoint to \"/opt/ml/model/model_algo_1-0000.params\"\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:39:18 INFO 139786247001920] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 7, \"sum\": 7.0, \"min\": 7}}, \"EndTime\": 1590719958.446942, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Object Detection\", \"epoch\": 6}, \"StartTime\": 1590719768.47262}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:39:58 INFO 139786247001920] Epoch:    7, batches:    100, num_examples:   1600, 39.7 samples/sec, epoch time so far:  0:00:40.336741\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:40:41 INFO 139786247001920] Epoch:    7, batches:    200, num_examples:   3200, 38.7 samples/sec, epoch time so far:  0:01:22.596250\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:41:23 INFO 139786247001920] Epoch:    7, batches:    300, num_examples:   4800, 38.3 samples/sec, epoch time so far:  0:02:05.455871\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:41:46 INFO 139786247001920] #quality_metric: host=algo-1, epoch=7, batch=355 train cross_entropy <loss>=(0.4192009874850213)\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:41:46 INFO 139786247001920] #quality_metric: host=algo-1, epoch=7, batch=355 train smooth_l1 <loss>=(0.4301388667950483)\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:41:46 INFO 139786247001920] Round of batches complete\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:41:46 INFO 139786247001920] Updated the metrics\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:42:23 INFO 139786247001920] #quality_metric: host=algo-1, epoch=7, validation mAP <score>=(0.5093264838950851)\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:42:23 INFO 139786247001920] Updating the best model with validation-mAP=0.5093264838950851\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:42:23 INFO 139786247001920] Saved checkpoint to \"/opt/ml/model/model_algo_1-0000.params\"\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:42:23 INFO 139786247001920] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}}, \"EndTime\": 1590720143.845121, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Object Detection\", \"epoch\": 7}, \"StartTime\": 1590719958.447169}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:43:07 INFO 139786247001920] Epoch:    8, batches:    100, num_examples:   1600, 36.7 samples/sec, epoch time so far:  0:00:43.646788\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:43:49 INFO 139786247001920] Epoch:    8, batches:    200, num_examples:   3200, 37.2 samples/sec, epoch time so far:  0:01:25.988596\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:44:32 INFO 139786247001920] Epoch:    8, batches:    300, num_examples:   4800, 37.2 samples/sec, epoch time so far:  0:02:08.976650\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:44:56 WARNING 139786247001920] Expected number of batches: 355, did not match the number of batches processed: 356. This may happen when some images or annotations are invalid and cannot be parsed. Please check the dataset and ensure it follows the format in the documentation.\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:44:56 INFO 139786247001920] #quality_metric: host=algo-1, epoch=8, batch=356 train cross_entropy <loss>=(0.4115528800431002)\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:44:56 INFO 139786247001920] #quality_metric: host=algo-1, epoch=8, batch=356 train smooth_l1 <loss>=(0.42120649865819304)\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:44:56 INFO 139786247001920] Round of batches complete\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:44:56 INFO 139786247001920] Updated the metrics\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:45:34 INFO 139786247001920] #quality_metric: host=algo-1, epoch=8, validation mAP <score>=(0.49755887657726144)\u001b[0m\n",
      "\u001b[34m[05/29/2020 02:45:34 INFO 139786247001920] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 9, \"sum\": 9.0, \"min\": 9}}, \"EndTime\": 1590720334.579338, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Object Detection\", \"epoch\": 8}, \"StartTime\": 1590720143.84534}\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "od_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_detector = od_model.deploy(initial_instance_count = 1,\n",
    "                                 instance_type = 'ml.m5.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detection(img_file, dets, classes=[], thresh=0.6):\n",
    "        \"\"\"\n",
    "        visualize detections in one image\n",
    "        Parameters:\n",
    "        ----------\n",
    "        img : numpy.array\n",
    "            image, in bgr format\n",
    "        dets : numpy.array\n",
    "            ssd detections, numpy.array([[id, score, x1, y1, x2, y2]...])\n",
    "            each row is one object\n",
    "        classes : tuple or list of str\n",
    "            class names\n",
    "        thresh : float\n",
    "            score threshold\n",
    "        \"\"\"\n",
    "        import random\n",
    "        import matplotlib.pyplot as plt\n",
    "        import matplotlib.image as mpimg\n",
    "\n",
    "        img=mpimg.imread(img_file)\n",
    "        plt.imshow(img)\n",
    "        height = img.shape[0]\n",
    "        width = img.shape[1]\n",
    "        colors = [\"green\",\"yellow\",\"red\"]\n",
    "\n",
    "        for det in dets:\n",
    "            (klass, score, x0, y0, x1, y1) = det\n",
    "            if score < thresh:\n",
    "                continue\n",
    "            cls_id = int(klass)\n",
    "\n",
    "            if cls_id == 0 or cls_id == 2:       \n",
    "                xmin = int(x0 * width)\n",
    "                ymin = int(y0 * height)\n",
    "                xmax = int(x1 * width)\n",
    "                ymax = int(y1 * height)\n",
    "                rect = plt.Rectangle((xmin, ymin), \n",
    "                                     xmax - xmin,\n",
    "                                     ymax - ymin, \n",
    "                                     fill=False,\n",
    "                                     edgecolor=colors[cls_id],\n",
    "                                     linewidth=1.0)\n",
    "                plt.gca().add_patch(rect)\n",
    "                class_name = str(cls_id)\n",
    "                if classes and len(classes) > cls_id:\n",
    "                    class_name = classes[cls_id]\n",
    "                plt.gca().text(xmin, ymin - 2,\n",
    "                                '{:s} {:.3f}'.format(class_name, score),\n",
    "                                bbox=dict(facecolor=colors[cls_id], alpha=0.5),\n",
    "                                        fontsize=12, color='white')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'test1.jpg'\n",
    "\n",
    "with open(file_name, 'rb') as image:\n",
    "    f = image.read()\n",
    "    b = bytearray(f)\n",
    "    ne = open('n.txt','wb')\n",
    "    ne.write(b)\n",
    "\n",
    "object_detector.content_type = 'image/jpeg'\n",
    "results = object_detector.predict(b)\n",
    "detections = json.loads(results)\n",
    "object_categories = ['hat', 'person']\n",
    "threshold = 0.50\n",
    "\n",
    "# Visualize the detections.\n",
    "visualize_detection(file_name, detections['prediction'], object_categories, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'test2.jpg'\n",
    "\n",
    "with open(file_name, 'rb') as image:\n",
    "    f = image.read()\n",
    "    b = bytearray(f)\n",
    "    ne = open('n.txt','wb')\n",
    "    ne.write(b)\n",
    "\n",
    "object_detector.content_type = 'image/jpeg'\n",
    "results = object_detector.predict(b)\n",
    "detections = json.loads(results)\n",
    "object_categories = ['hat', 'person']\n",
    "threshold = 0.50\n",
    "\n",
    "# Visualize the detections.\n",
    "visualize_detection(file_name, detections['prediction'], object_categories, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'test3.jpg'\n",
    "\n",
    "with open(file_name, 'rb') as image:\n",
    "    f = image.read()\n",
    "    b = bytearray(f)\n",
    "    ne = open('n.txt','wb')\n",
    "    ne.write(b)\n",
    "\n",
    "object_detector.content_type = 'image/jpeg'\n",
    "results = object_detector.predict(b)\n",
    "detections = json.loads(results)\n",
    "object_categories = ['hat', 'person']\n",
    "threshold = 0.50\n",
    "\n",
    "# Visualize the detections.\n",
    "visualize_detection(file_name, detections['prediction'], object_categories, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
